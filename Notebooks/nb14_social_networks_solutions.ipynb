{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 14: Social Networks\n",
    "***\n",
    "\n",
    "In this notebook we'll work through a few of the key steps in characterizing structure within a social network. Namely, we can unpack some of the mechanics surrounding estimating the community membership strength matrix $F$, and the steps necessary in order to estimate the elements of $F$ using gradient ascent.\n",
    "\n",
    "We'll need numpy for this notebook, so let's load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 1: Computing connection probabilities\n",
    "\n",
    "Suppose there are 4 college students who are enrolled in different courses. As any of you, who are all in the same class - *this most glorious of classes* - are aware, being enrolled in similar/same classes has an effect on whether or not students are connected to one another. The students' membership strengths in those class communities are given below.\n",
    "\n",
    "$$\\begin{array}{c|cccc}\n",
    "   & \\text{Calculus IV} & \\text{Intro to Pseudoscience} & \\text{Remedial Recess} & \\text{Advanced Living} \\\\\n",
    "   \\hline\n",
    " \\text{Jake}  & 0   & 0.2 & 0.6 & 0.8 \\\\\n",
    " \\text{Bri}   & 0.1 & 0.3 & 0.4 & 0.9 \\\\\n",
    " \\text{David} & 1   & 0.4 & 0.5 & 0.4 \\\\\n",
    " \\text{Alaina}& 0.8 & 0.1 & 0.7 & 0.5 \\\\\n",
    " \\end{array}$$\n",
    " \n",
    "Write down and simplify as much as possible expressions for the following connection probabilities. Then, compute them in Python.\n",
    "\n",
    "1. $P(Jake, Bri)$\n",
    "1. $P(Jake, David)$\n",
    "1. $P(David, Alaina)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Jake, Bri) = 0.6394\n",
      "P(Jake, David) = 0.5034\n",
      "P(David, Alaina) = 0.7509\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "F = np.array([[0  ,0.2,0.6,0.8],\n",
    "              [0.1,0.3,0.4,0.9],\n",
    "              [1.0,0.4,0.5,0.4],\n",
    "              [0.8,0.1,0.7,0.5]])\n",
    "\n",
    "print(\"P(Jake, Bri) = {:0.4f}\".format(1 - np.exp(-np.sum(F[0,:]*F[1,:]))))\n",
    "print(\"P(Jake, David) = {:0.4f}\".format(1 - np.exp(-np.sum(F[0,:]*F[2,:]))))\n",
    "print(\"P(David, Alaina) = {:0.4f}\".format(1 - np.exp(-np.sum(F[2,:]*F[3,:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 2: Log-likelihood is the queen of functions\n",
    "\n",
    "In reality, we would mine a network graph, say, from Facebook, LinkedIn, or some other networking site. Then, we would use that if estimate the elements of the community membership strength matrix, $F$, by choosing the elements of $F$ to maximize the log-likelihood:\n",
    "$$l(F) = \\sum_{(u,v)\\in E} \\log{(1 - \\exp{(-\\vec{F}_u\\cdot \\vec{F}_v)})} - \\sum_{(u,v)\\notin E} \\vec{F}_u\\cdot \\vec{F}_v$$\n",
    "where $E$ is the edge set for the graph (which we assume to be undirected).\n",
    "\n",
    "Suppose our network graph actually looks like this:\n",
    "\n",
    "<img width=130px src=\"http://www.cs.colorado.edu/~anwo7157/home/resources/socialnetwork1.png\">\n",
    "\n",
    "What then is the log-likelihood for the $F$ community strength matrix given in Exercise 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume same order as the rows of F: J, B, D, A\n",
    "J = 0; B = 1; D = 2; A = 3\n",
    "\n",
    "# Edge set\n",
    "E = [[J, B], [J, D], [J, A], [D, A]]\n",
    "\n",
    "# All edges\n",
    "EU = [[J, B], [J, D], [J, A], [B, D], [B, A], [D, A]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.605132384756333\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "llhood = np.sum([np.log(1-np.exp(-np.sum(F[edge[0],:]*F[edge[1],:]))) for edge in E]) - \\\n",
    "         np.sum([np.sum(F[edge[0],:]*F[edge[1],:]) if edge not in E else 0 for edge in EU])\n",
    "print(llhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you play around with any of the elements of $F$ in order to increase the log-likelihood value? What does an increase in the log-likelihood mean, in terms of how the community membership strength matrix represents the given graph? Can you use some of the structure of the graph to justify the change you make in the $F$ matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Sure! In the graph, Jake and Alaina are connected, but Alaina has a high membership strength in Calculus IV, whereas Jake has 0 membership strength in that class community. So, if we increase Jake's membership strength there, the log-likelihood should increase, since the membership strength matrix better represents what is shown in the graph connections.\n",
    "\n",
    "So, we just increase Jake's membership strength in Calculus IV from 0 to 0.5 and voila!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.026539933429717\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "F = np.array([[0.5,0.2,0.6,0.8],\n",
    "              [0.1,0.3,0.4,0.9],\n",
    "              [1.0,0.4,0.5,0.4],\n",
    "              [0.8,0.1,0.7,0.5]])\n",
    "llhood = np.sum([np.log(1-np.exp(-np.sum(F[edge[0],:]*F[edge[1],:]))) for edge in E]) - \\\n",
    "         np.sum([np.sum(F[edge[0],:]*F[edge[1],:]) if edge not in E else 0 for edge in EU])\n",
    "print(llhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 3: Gradient ascent\n",
    "\n",
    "In order to choose components of $F$ to maximize the log-likelihood function, we use **gradient ascent**. A crucial part of gradient ascent is to be able to compute $\\nabla l(F_u)$ the gradient of $l(F)$ with respect to the row of $F$ corresponding to node $u$. So, taking David as an example, the gradient of the log-likelihood with respect to the components in David's row is:\n",
    "\n",
    "$$\\nabla l(F_D) = \\bigg\\langle \\sum_{v \\in N(D)} F_{v,A} \\dfrac{\\exp{(-\\vec{F}_D \\cdot \\vec{F}_v)}}{1-\\exp{(-\\vec{F}_D \\cdot \\vec{F}_v)}} - \\sum_{v \\notin N(D)} F_{v,A}, \\sum_{v \\in N(D)} F_{v,B} \\dfrac{\\exp{(-\\vec{F}_D \\cdot \\vec{F}_v)}}{1-\\exp{(-\\vec{F}_D \\cdot \\vec{F}_v)}} - \\sum_{v \\notin N(D)} F_{v,B}, \\ldots \\bigg\\rangle,$$\n",
    "\n",
    "where $A$, $B$, ... represent each of the communities (classes) and $N(D)$ is the set of neighbor nodes of David (Alaina and Jake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = [A,J]\n",
    "not_neighbors = [B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking our original $F$ matrix from the first exercise, let's compute the gradient with respect to David's row, and update his row using step size (learning rate) $\\eta = 0.1$. We'll compute the first component (with respect to David and Calculus IV) together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16535316381837276\n"
     ]
    }
   ],
   "source": [
    "F = np.array([[0  ,0.2,0.6,0.8],\n",
    "              [0.1,0.3,0.4,0.9],\n",
    "              [1.0,0.4,0.5,0.4],\n",
    "              [0.8,0.1,0.7,0.5]])\n",
    "\n",
    "first_sum = np.sum([F[v,0]*np.exp(-np.sum(F[D,:]*F[v,:]))/(1-np.exp(-np.sum(F[D,:]*F[v,:]))) for v in neighbors])\n",
    "second_sum = np.sum([F[v,0] for v in not_neighbors])\n",
    "the_whole_enchilada = first_sum - second_sum\n",
    "print(the_whole_enchilada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the other components of $\\nabla l(F_D)$, the ones with respect to the other class communities. It might be a good idea to store them all in a well-named array, instead of silly variable names that are easy to forget but funny to say. Note that we have the Calculus IV component of the gradient from above, so you can check your work using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16535316 -0.06954408  0.42404434  0.05499282]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# initialize some useful things\n",
    "grad_llhood = np.zeros(4)  # one log-likelihood gradient value for each community\n",
    "\n",
    "for comm in range(4):\n",
    "    first_sum = np.sum([F[v,comm]*np.exp(-np.sum(F[D,:]*F[v,:]))/(1-np.exp(-np.sum(F[D,:]*F[v,:]))) for v in neighbors])\n",
    "    second_sum = np.sum([F[v,comm] for v in not_neighbors])\n",
    "    grad_llhood[comm] = first_sum - second_sum\n",
    "\n",
    "print(grad_llhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now update the David row of $F$ as $F_D^{new} = F_D^{old} + \\eta\\cdot \\nabla l(F_D)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_old: [[0.  0.2 0.6 0.8]\n",
      " [0.1 0.3 0.4 0.9]\n",
      " [1.  0.4 0.5 0.4]\n",
      " [0.8 0.1 0.7 0.5]]\n",
      "F_new: [[0.     0.2    0.6    0.8   ]\n",
      " [0.1    0.3    0.4    0.9   ]\n",
      " [1.0165 0.393  0.5424 0.4055]\n",
      " [0.8    0.1    0.7    0.5   ]]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "F_new = F.copy()\n",
    "F_new[D,:] += eta*grad_llhood\n",
    "print(\"F_old:\", np.round(F,4))\n",
    "print(\"F_new:\", np.round(F_new,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, update the remaining rows of $F$. Some notes:\n",
    "* If any elements of $F$ are updated to yield membership strengths < 0, those should be reset to 0. \n",
    "* You will need to make sure you change the `neighbors` and `not_neighbors` sets with each new row of $F$ you are updating.\n",
    "\n",
    "With that, you will have performed one full iteration of gradient ascent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_old: [[0.  0.2 0.6 0.8]\n",
      " [0.1 0.3 0.4 0.9]\n",
      " [1.  0.4 0.5 0.4]\n",
      " [0.8 0.1 0.7 0.5]]\n",
      "F_new: [[0.1651 0.244  0.6651 0.8482]\n",
      " [0.     0.2313 0.2738 0.7651]\n",
      " [1.     0.4    0.5    0.4   ]\n",
      " [0.7432 0.0885 0.6522 0.434 ]]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "F_new = F.copy()\n",
    "\n",
    "nodes = [J,B,D,A]\n",
    "neighbors = [[B,D,A], [J], [A,J], [D,J]]\n",
    "\n",
    "print(\"F_old:\", np.round(F_new,4))\n",
    "\n",
    "for u in [J,B,A]:\n",
    "    not_neighbors = list(set(nodes)-set(neighbors[u]))\n",
    "    for comm in range(4):\n",
    "        first_sum = np.sum([F[v,comm]*np.exp(-np.sum(F[u,:]*F[v,:]))/(1-np.exp(-np.sum(F[u,:]*F[v,:]))) for v in neighbors[u]])\n",
    "        second_sum = np.sum([F[v,comm] for v in not_neighbors])\n",
    "        grad_llhood[comm] = first_sum - second_sum\n",
    "        #print (u, comm, grad_llhood[comm])\n",
    "\n",
    "    F_new[u,:] += eta*grad_llhood\n",
    "    if any(F_new[u,:] < 0):\n",
    "        F_new[u,:] = [max([F_new[B,k], 0]) for k in range(4)]\n",
    "\n",
    "print(\"F_new:\", np.round(F_new,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our update didn't follow the order of the nodes in the matrix. That's fine, it turns out it's like a watered down version of *stochastic* gradient ascent. If the order in which we update the $F$ matrix makes a big difference, then we have bigger problems to worry about.\n",
    "\n",
    "So what happens if we take 100 steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_old: [[0.  0.2 0.6 0.8]\n",
      " [0.1 0.3 0.4 0.9]\n",
      " [1.  0.4 0.5 0.4]\n",
      " [0.8 0.1 0.7 0.5]]\n",
      "F_new: [[0.1651 0.244  0.6651 0.8482]\n",
      " [0.     0.2313 0.2738 0.7651]\n",
      " [0.9165 0.353  0.4924 0.3655]\n",
      " [0.7432 0.0885 0.6522 0.434 ]]\n",
      "F_new: [[0.3301 0.2879 0.7301 0.8964]\n",
      " [0.     0.1626 0.1477 0.6302]\n",
      " [0.8331 0.3061 0.4848 0.331 ]\n",
      " [0.6863 0.0769 0.6043 0.3681]]\n",
      "F_new: [[0.4952 0.3319 0.7952 0.9446]\n",
      " [0.     0.0938 0.0215 0.4953]\n",
      " [0.7496 0.2591 0.4772 0.2965]\n",
      " [0.6295 0.0654 0.5565 0.3021]]\n",
      "F_new: [[0.6602 0.3759 0.8602 0.9928]\n",
      " [0.     0.0251 0.     0.3605]\n",
      " [0.6661 0.2122 0.4696 0.262 ]\n",
      " [0.5727 0.0538 0.5087 0.2362]]\n",
      "F_new: [[0.8253 0.4199 0.9253 1.041 ]\n",
      " [0.     0.     0.     0.2256]\n",
      " [0.5827 0.1652 0.462  0.2275]\n",
      " [0.5158 0.0423 0.4608 0.1702]]\n",
      "F_new: [[0.9903 0.4638 0.9903 1.0892]\n",
      " [0.     0.     0.     0.0907]\n",
      " [0.4992 0.1183 0.4544 0.193 ]\n",
      " [0.459  0.0308 0.413  0.1042]]\n",
      "F_new: [[1.1554 0.5078 1.0554 1.1374]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.4157 0.0713 0.4468 0.1585]\n",
      " [0.4022 0.0192 0.3652 0.0383]]\n",
      "F_new: [[1.3205 0.5518 1.1205 1.1856]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.3323 0.0244 0.4392 0.124 ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[1.4855 0.5958 1.1855 1.2338]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[1.6506 0.6397 1.2506 1.282 ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[1.8156 0.6837 1.3156 1.3302]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[1.9807 0.7277 1.3807 1.3784]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[2.1457 0.7716 1.4457 1.4266]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[2.3108 0.8156 1.5108 1.4748]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[2.4758 0.8596 1.5758 1.5229]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[2.6409 0.9036 1.6409 1.5711]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[2.806  0.9475 1.706  1.6193]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[2.971  0.9915 1.771  1.6675]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[3.1361 1.0355 1.8361 1.7157]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[3.3011 1.0795 1.9011 1.7639]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[3.4662 1.1234 1.9662 1.8121]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[3.6312 1.1674 2.0312 1.8603]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[3.7963 1.2114 2.0963 1.9085]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[3.9614 1.2553 2.1614 1.9567]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[4.1264 1.2993 2.2264 2.0049]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[4.2915 1.3433 2.2915 2.0531]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[4.4565 1.3873 2.3565 2.1013]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[4.6216 1.4312 2.4216 2.1495]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[4.7866 1.4752 2.4866 2.1977]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[4.9517 1.5192 2.5517 2.2459]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[5.1167 1.5632 2.6168 2.2941]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[5.2818 1.6071 2.6818 2.3423]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[5.4469 1.6511 2.7469 2.3905]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[5.6119 1.6951 2.8119 2.4387]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[5.777  1.739  2.877  2.4869]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[5.942  1.783  2.942  2.5351]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[6.1071 1.827  3.0071 2.5833]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[6.2721 1.871  3.0721 2.6315]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[6.4372 1.9149 3.1372 2.6797]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[6.6023 1.9589 3.2023 2.7279]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[6.7673 2.0029 3.2673 2.7761]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[6.9324 2.0469 3.3324 2.8243]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[7.0974 2.0908 3.3974 2.8725]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[7.2625 2.1348 3.4625 2.9206]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[7.4275 2.1788 3.5275 2.9688]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[7.5926 2.2227 3.5926 3.017 ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[7.7576 2.2667 3.6577 3.0652]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[7.9227 2.3107 3.7227 3.1134]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[8.0878 2.3547 3.7878 3.1616]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[8.2528 2.3986 3.8528 3.2098]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[8.4179 2.4426 3.9179 3.258 ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[8.5829 2.4866 3.9829 3.3062]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[8.748  2.5305 4.048  3.3544]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[8.913  2.5745 4.1131 3.4026]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[9.0781 2.6185 4.1781 3.4508]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[9.2432 2.6625 4.2432 3.499 ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[9.4082 2.7064 4.3082 3.5472]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[9.5733 2.7504 4.3733 3.5954]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[9.7383 2.7944 4.4383 3.6436]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[9.9034 2.8384 4.5034 3.6918]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "F_new: [[10.0684  2.8823  4.5684  3.74  ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[10.2335  2.9263  4.6335  3.7882]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[10.3985  2.9703  4.6986  3.8364]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[10.5636  3.0142  4.7636  3.8846]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[10.7287  3.0582  4.8287  3.9328]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[10.8937  3.1022  4.8937  3.981 ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[11.0588  3.1462  4.9588  4.0292]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[11.2238  3.1901  5.0238  4.0774]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[11.3889  3.2341  5.0889  4.1256]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_new: [[11.5539  3.2781  5.154   4.1738]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[11.719   3.3221  5.219   4.222 ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[11.8841  3.366   5.2841  4.2702]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[12.0491  3.41    5.3491  4.3183]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[12.2142  3.454   5.4142  4.3665]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[12.3792  3.4979  5.4792  4.4147]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[12.5443  3.5419  5.5443  4.4629]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[12.7093  3.5859  5.6094  4.5111]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[12.8744  3.6299  5.6744  4.5593]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[13.0394  3.6738  5.7395  4.6075]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[13.2045  3.7178  5.8045  4.6557]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[13.3696  3.7618  5.8696  4.7039]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[13.5346  3.8058  5.9346  4.7521]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[13.6997  3.8497  5.9997  4.8003]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[13.8647  3.8937  6.0647  4.8485]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[14.0298  3.9377  6.1298  4.8967]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[14.1948  3.9816  6.1949  4.9449]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[14.3599  4.0256  6.2599  4.9931]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[14.525   4.0696  6.325   5.0413]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[14.69    4.1136  6.39    5.0895]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[14.8551  4.1575  6.4551  5.1377]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[15.0201  4.2015  6.5201  5.1859]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[15.1852  4.2455  6.5852  5.2341]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[15.3502  4.2895  6.6503  5.2823]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[15.5153  4.3334  6.7153  5.3305]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[15.6803  4.3774  6.7804  5.3787]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[15.8454  4.4214  6.8454  5.4269]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[16.0105  4.4653  6.9105  5.4751]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[16.1755  4.5093  6.9755  5.5233]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[16.3406  4.5533  7.0406  5.5715]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "F_new: [[16.5056  4.5973  7.1057  5.6197]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "F_new = F.copy()\n",
    "\n",
    "nodes = [J,B,D,A]\n",
    "neighbors = [[B,D,A], [J], [A,J], [D,J]]\n",
    "\n",
    "print(\"F_old:\", np.round(F_new,4))\n",
    "\n",
    "for steps in range(100):\n",
    "    for u in [D,J,B,A]:\n",
    "        not_neighbors = list(set(nodes)-set(neighbors[u]))\n",
    "        for comm in range(4):\n",
    "            first_sum = np.sum([F[v,comm]*(np.exp(-np.sum(F[u,:]*F[v,:]))/(1-np.exp(-np.sum(F[u,:]*F[v,:])))) for v in neighbors[u]])\n",
    "            second_sum = np.sum([F[v,comm] for v in not_neighbors])\n",
    "            grad_llhood[comm] = first_sum - second_sum\n",
    "            #print (u, comm, grad_llhood[comm])\n",
    "\n",
    "        F_new[u,:] += eta*grad_llhood\n",
    "        if any(F_new[u,:] < 0):\n",
    "            F_new[u,:] = [max([F_new[B,k], 0]) for k in range(4)]\n",
    "    print(\"F_new:\", np.round(F_new,4))\n",
    "# print(\"F_new:\", np.round(F_new,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... oh no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: A little trip down \"best practices\" lane.\n",
    "\n",
    "In fall 2020 the course CSCI 4022 was offered for the first time.  Dr. Zach said: we've done BigCLAM, let's give them a cool example!  It was [this](https://www.kaggle.com/csanhueza/the-marvel-universe-social-network) data set of all the Marvel comics.  Big data, lots of connections, invites both regular BigCLAM and *weighted* BigClam, where the size of the update is proportional to the **number of times** an edge or connection appears.\n",
    "\n",
    "One minor problem: the algorithm as presented was pretty insufficient to tackle this problem.  Because for large, complicated data science techniques, we may need more than a toy example.  And also, specific problems might hit us for **large** data sets that don't for small data sets.\n",
    "\n",
    "So what went wrong?\n",
    "\n",
    "1) It's actually important to have a \"background\" edge probability.  This could be the probability of \"cameos\" or \"one-offs\" in the Marvel universe, or \"happenstance\" network connections.  Or even just \"there were actually more communities than I put in my model, so I should have some edges coming from *those*\".  In essence: $F_u \\cdot F_v$ should *never* be zero.  Instead, we would have it increase by $\\varepsilon^2$ for every $u$ and $v$, whether they share communities or not.  In practice, this means that the update step is both the community update and a (quite small) step towards *all* communities.\n",
    "\n",
    "2) Initialization matters.  A lot.  Here's a thought exercise.  Imagine we have a graph with 1000 nodes and each has 10 edges.  We use *random initialization*: everyone starts with a random uniform number from 0-1.  What happens?   Try to work it out from the summand:\n",
    "$$(2.4) \\quad \\nabla l(F_u) =\\sum_{v \\in N(u)} F_{v} \\left(\\dfrac{\\exp{(-\\vec{F}_u \\cdot \\vec{F}_v)}}{1-\\exp{(-\\vec{F}_u \\cdot \\vec{F}_v)}} + 1\\right) -  \\sum_{\\text{All } v} F_{v}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: The communities *on average* have \"500\" community affiliation in total.  But any given person and their neighbors have *very little* of that.  Consider user $u$.  Then for every community, most of the community affiliation exists in the 990 non-neighbors of $u$.  So when we update $u$, we think they're *not members of any community*: the subtract sign contains far more of each communities total than the addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we fix these sorts of things?  We read!  We find other examples, or the spot there are *always* examples.  \n",
    "\n",
    "**Read primary sources**.  Don't just trust slides or Wikipedia.  Peer-reviewed literature is a good thing.  [Here's](https://cs.stanford.edu/people/jure/pubs/bigclam-wsdm13.pdf) the original paper for BigCLAM.\n",
    "\n",
    "Two crucial snippets:\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "To initialize F, we use locally minimal neighborhoods [11]. Neighborhoods N(u) of node u is a community of u\n",
    "and its neighbors, and N(u) is locally minimal if N(u) has lower\n",
    "conductance than all the N(v) for nodes v who are connected to\n",
    "u. Recently, Gleich et al. [11] empirically showed that the locally\n",
    "minimal neighborhoods are good seed sets for community detection algorithms. For a node u\n",
    "0 who belongs to a locally minimal\n",
    "neighborhood k, we initialize $F_{u', k}= 1$, otherwise $F_{u', k}= 0$\n",
    "</blockquote>\n",
    "\n",
    "and\n",
    "<blockquote>\n",
    "\n",
    "ε-Community. In the formulation of Equation 1, BIGCLAM does\n",
    "not allow for the edges between the nodes u and v that do not share\n",
    "any common communities since for such nodes Fuc · Fvc = 0\n",
    "for all c. To allow for edges between nodes that do not share\n",
    "any community affiliations, we assume an additional community,\n",
    "called the ε-community, which connects any pair of nodes with a\n",
    "very small probability ε. We find that setting ε to be the background edge probability between a random pair of nodes (ε =\n",
    "2|E|/|V |(|V |−1)) works well in practice. For all our experiments\n",
    "we set ε ≈ 10−8\n",
    "</blockquote>\n",
    "\n",
    "Note that the textbook **also** does not include either of these discussions.  Gross!\n",
    "\n",
    "In practice, we want an *informed* initialization for BigCLAM:  Start the community centers somewhere - $k$ different people - and let the probabilties/affiliation diffuse out from those people.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Exercise 5: Are you your own neighbor?  (and BigCLAM with sparsity)\n",
    "\n",
    "**It turns out there are some numerical challenges with BigCLAM...** \n",
    "let's discuss the gory details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the following example:\n",
    "\n",
    "<img width=130px src=\"http://www.cs.colorado.edu/~anwo7157/home/resources/socialnetwork1.png\">\n",
    "\n",
    "which was initialized with: \n",
    "$$\\begin{array}{c|cccc}\n",
    "   & \\text{Calculus IV} & \\text{Intro to Pseudoscience} & \\text{Remedial Recess} & \\text{Advanced Living} \\\\\n",
    "   \\hline\n",
    " \\text{Jake}  & 0   & 0.2 & 0.6 & 0.8 \\\\\n",
    " \\text{Bri}   & 0.1 & 0.3 & 0.4 & 0.9 \\\\\n",
    " \\text{David} & 1   & 0.4 & 0.5 & 0.4 \\\\\n",
    " \\text{Alaina}& 0.8 & 0.1 & 0.7 & 0.5 \\\\\n",
    " \\end{array}$$\n",
    "\n",
    "\n",
    "Our goal is to perform **gradient ascent**.\n",
    "\n",
    "We sketched out and fully simplified that algorithm in lecture 14 (see slide 35 and prior), which in pseudocode involved:\n",
    "\n",
    "1) Initialize F\n",
    "\n",
    "2) For each user $u$, calculate the gradient $\\nabla l(F_u)$, which involved summing over all of the neighbors $N(u)$ of $u$.\n",
    "\n",
    "3) Take a step of size $\\eta$ in the direction of $\\nabla l(F_u)$, then set any negative values to 0.\n",
    "\n",
    "4) Repeat steps 2-3) until convergence in F.\n",
    "\n",
    "\n",
    "Let's elaborate on step 2.  The unsimplified vectorized form was \n",
    "\n",
    "$$(2.1) \\quad \\nabla l(F_u) =\\sum_{v \\in N(u)} F_{v} \\dfrac{\\exp{(-\\vec{F}_u \\cdot \\vec{F}_v)}}{1-\\exp{(-\\vec{F}_u \\cdot \\vec{F}_v)}} - \\sum_{v \\notin N(u)} F_{v}$$\n",
    "\n",
    "We simplified this by using the fact that \n",
    "\n",
    "$$(2.2) \\quad \\sum_{\\text{All } v} F_{v} =\\sum_{v \\notin N(u)} F_{v}+ \\sum_{v \\in N(u)} F_{v}, $$\n",
    "\n",
    "so solving for $\\sum_{v \\in N(u)} F_{v}$ lets us rewrite the last sum in (2.1) as\n",
    "\n",
    "$$(2.3) \\quad \\sum_{v \\notin N(u)} F_{v}= \\sum_{\\text{All } v} F_{v}-\\sum_{v \\in N(u)} F_{v} $$\n",
    "\n",
    "which can be combined with the first sum into the form\n",
    "\n",
    "$$(2.4) \\quad \\nabla l(F_u) =\\sum_{v \\in N(u)} F_{v} \\left(\\dfrac{\\exp{(-\\vec{F}_u \\cdot \\vec{F}_v)}}{1-\\exp{(-\\vec{F}_u \\cdot \\vec{F}_v)}} + 1\\right) -  \\sum_{\\text{All } v} F_{v}=\\sum_{v \\in N(u)} F_{v} \\left(\\dfrac{1}{1-\\exp{(-\\vec{F}_u \\cdot \\vec{F}_v)}}\\right) -  \\sum_{\\text{All } v} F_{v}$$\n",
    "\n",
    "Our solution slide had an extra copy of $F_u$ floating around, however, based on whether or not you considered $u$ a member of $v \\notin N(u)$, a member of $v \\in N(u)$, or *neither*.  The last case - neither - was the notation on the slides.  \n",
    "\n",
    "\n",
    "$$(2.5; slides) \\quad \t \\nabla l(F_u)=\\sum_{v \\in N(u)} F_{v}\\left(  \\frac{1}{1-\\exp(-F_{\\boldsymbol{u}}\\cdot F_{\\boldsymbol{v}})} \\right) +F_u-\\sum_{v} F_{\\boldsymbol{v}}$$\n",
    "\n",
    "This led to some weird behavior in our code.  Here is the nb14 example passed into a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_old: [[0.  0.2 0.6 0.8]\n",
      " [0.1 0.3 0.4 0.9]\n",
      " [1.  0.4 0.5 0.4]\n",
      " [0.8 0.1 0.7 0.5]]\n",
      "F_new: [[16.5056  4.5973  7.1057  5.6197]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "##SETUP\n",
    "\n",
    "# Assume same order as the rows of F: J, B, D, A\n",
    "J = 0; B = 1; D = 2; A = 3\n",
    "\n",
    "# Edge set\n",
    "E = [[J, B], [J, D], [J, A], [D, A]]\n",
    "\n",
    "# All possible edges\n",
    "EU = [[J, B], [J, D], [J, A], [B, D], [B, A], [D, A]]\n",
    "\n",
    "F = np.array([[0  ,0.2,0.6,0.8],\n",
    "              [0.1,0.3,0.4,0.9],\n",
    "              [1.0,0.4,0.5,0.4],\n",
    "              [0.8,0.1,0.7,0.5]])\n",
    "nodes = [J,B,D,A]\n",
    "neighbors = [[B,D,A], [J], [A,J], [D,J]]\n",
    "\n",
    "F_new = F.copy()\n",
    "eta = 0.1\n",
    "\n",
    "grad_llhood = np.zeros(4)  # one log-likelihood gradient value for each community\n",
    "\n",
    "print(\"F_old:\", np.round(F_new,4))\n",
    "\n",
    "# Solution as a 100-step loop.:\n",
    "for nloops in range(100):\n",
    "    for u in [J,B,A,D]:\n",
    "        not_neighbors = list(set(nodes)-set(neighbors[u]))\n",
    "        #print(u, neighbors[u], not_neighbors)\n",
    "        for comm in range(4):\n",
    "            first_sum = np.sum([F[v,comm]*np.exp(-np.sum(F[u,:]*F[v,:]))/(1-np.exp(-np.sum(F[u,:]*F[v,:]))) for v in neighbors[u]])\n",
    "            second_sum = np.sum([F[v,comm] for v in not_neighbors])\n",
    "            grad_llhood[comm] = first_sum - second_sum\n",
    "            #print (u, comm, grad_llhood[comm])\n",
    "\n",
    "        F_new[u,:] += eta*grad_llhood\n",
    "        if any(F_new[u,:] < 0):\n",
    "            F_new[u,:] = [max([F_new[u,k], 0]) for k in range(4)]\n",
    "\n",
    "print(\"F_new:\", np.round(F_new,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no good!  Something is wrong, where either we're subtracting too much each step (making things get set to zero) or something else.  Rather than fully debug the above code (which uses the version in (2.1), above), I tried rewriting the algorithm using the form in (2.4).\n",
    "\n",
    "Here is my example graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c932ef4e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PICTURE OF EXAMPLE\n",
    "#Example: cluster 0-2, cluster of 3-5, cluster of 6-9, random initialization.\n",
    "G=nx.Graph()\n",
    "edges=[[1,2],[0,2],[0,1,3],\\\n",
    "       [2,4,5],[3,5],[3,4,6],\\\n",
    "       [5,7,8,9],[6,8,9],[6,7,9],[6,7,8]]\n",
    "nodes=[i for i in range(10)]\n",
    "G.add_nodes_from(nodes)\n",
    "for i in range(10):\n",
    "    toadd=edges[i]\n",
    "    for j in range(len(toadd)):\n",
    "        G.add_edge(i,toadd[j])\n",
    "        \n",
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is BigCLAM, using a random initialization for F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.46103188 0.         0.        ]\n",
      " [1.46012614 0.         0.        ]\n",
      " [9.33371269 0.         0.        ]\n",
      " [0.76034951 1.48707302 0.        ]\n",
      " [0.         1.55666017 0.        ]\n",
      " [0.         9.35907419 0.        ]\n",
      " [0.         0.6952536  2.94591653]\n",
      " [0.         0.         2.94868911]\n",
      " [0.         0.         2.95277449]\n",
      " [0.         0.         2.94794287]]\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(2039)\n",
    "F=np.random.rand(10,3)\n",
    "\n",
    "comm_sums=F.sum(0) #sum over all communities, cached for updating in later version\n",
    "eta=.1 #step size\n",
    "for lezgo in range(1000): #number of iterations\n",
    "    for i in nodes: #loop over rows of F\n",
    "        F_grad=np.zeros(3) #initialize gradient\n",
    "        comm_sums=F.sum(0) #should be cached, will deal with later since it's annoying to update correctly with the 0-check\n",
    "        for j in edges[i]: #loop over nbrs of that row's node\n",
    "                IP=np.exp(-np.inner(F[i], F[j])) #so we don't have to double compute\n",
    "                F_grad+=F[j]*(1/(1-IP)) ##\"first sum\" above.\n",
    "#                 print(i,j,F_grad)\n",
    "        step=eta*(F_grad-comm_sums) #including F[i] here is the difference between you-are-own-neighbor and not\n",
    "#         step=eta*(F_grad-comm_sums-F[i]) #correct option; above DIVERGES as steps->infty\n",
    "        F[i]+=step #update row; doing as two steps so can use \"step\" to update comm_sums later\n",
    "        F[i,F[i,:]<0]=0 #0-check\n",
    "#         print('i',i,'step:', F_grad, 'baseline:',comm_sums) # sanity checks\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three things to try for your own use: \n",
    "\n",
    "1) Try this loop with a larger step size (Try `eta`=$\\eta=.2$).  It may diverge.  This is a little scary!\n",
    "    \n",
    "2) Try this loop with the extra F(i) commented in *inside* the parenthesis of the `step=...` line.  This would match the slides.  It won't work!\n",
    "    \n",
    "3) When you use this in practice, you probably should adjust the `comm_sums` line if you want to not recompute that every step of the way.  Ideally you could just update with `comm_sums+= step` near the end... but this doesn't work if you end up changing the `comm_sums` by setting negative entries equal to zero.  It's only one line to fix, but think about it some!\n",
    "\n",
    "NB: If we had included $[i,i]$ as an edge for every i, we should include the `-F[i])` inside the step parenthetical, and the algorithm seems to work, with our notation closer to that of the book.  See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.06892268 0.        ]\n",
      " [0.         1.06892268 0.        ]\n",
      " [0.         1.40198207 0.        ]\n",
      " [1.04821419 0.33585329 0.        ]\n",
      " [1.07200033 0.         0.        ]\n",
      " [1.40435033 0.         0.        ]\n",
      " [0.33071145 0.         1.25580956]\n",
      " [0.         0.         1.26950034]\n",
      " [0.         0.         1.26950034]\n",
      " [0.         0.         1.26950034]]\n"
     ]
    }
   ],
   "source": [
    "edges=[[0,1,2],[0,1,2],[0,1,2,3],\\\n",
    "       [2,3,4,5],[3,4,5],[3,4,5,6],\\\n",
    "       [5,6,7,8,9],[6,7,8,9],[6,7,8,9],[6,7,8,9]]\n",
    "nodes=[i for i in range(10)]\n",
    "\n",
    "np.random.seed(2039)\n",
    "F=np.random.rand(10,3)\n",
    "\n",
    "comm_sums=F.sum(0) #sum over all communities, cached for updating in later version\n",
    "nu=.1 #step size\n",
    "for lezgo in range(3000): #number of iterations\n",
    "    for i in nodes: #loop over rows of F\n",
    "        F_grad=np.zeros(3) #initialize gradient\n",
    "        comm_sums=F.sum(0) #should be cached, will deal with later since it's annoying to update correctly with the 0-check\n",
    "        for j in edges[i]: #loop over nbrs of that row's node\n",
    "                IP=np.exp(-np.inner(F[i], F[j])) #so we don't have to double compute\n",
    "                F_grad+=F[j]*(IP/(1-IP)+1) ##\"first sum\" below.\n",
    "                #print(F_grad)\n",
    "        step=nu*(F_grad-comm_sums-F[i]) #\n",
    "        F[i]+=step #update row; doing as two steps so can use \"step\" to update comm_sums later\n",
    "        F[i,F[i,:]<0]=0 #0-check\n",
    "        #print('i',i,'step:', F_grad, 'baseline:',comm_sums) # sanity checks\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should get almost the exact same values at convergence here as the variant using the \"-F[u]\" in the cells above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
